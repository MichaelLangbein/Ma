\section{General algorithm theory}

\subsection{Invariant principle}

The invariant principle is not much more than using induction to prove that some statement holds over all iterations $n$ from $0$ to $n_0$.


\subsection{Well ordering}

The well ordering principle is a fundamental theorem in discrete mathematics. 

$$ \forall S \subset \naturals^+: S \neq \emptyset : \thereis x_0 \in S: \forall x \in S: x_0 \leq x $$

We usually use it in existence proofs, by contradiction. However, there is a nice little template for proofs using the well ordering principle.

To prove that $P(n)$ holds true for $\forall n \in \naturals$:

\begin{itemize}
    \item Define the set $C$ of counterexamples to $P$ being true. That is: 
    $$ C = \{ n \in \naturals | \lnot P(n) \}$$
    
    \item For a proof by contradiction, assume $C$ is nonempty.
    
    \item By the well ordering principle, there will be a smallest element $n_0 \in C$
    
    \item Reach a contradiction (often by showing how to use $n_0$ to find another member of $n_1 \in C$ that is even smaller than $n_0$)
\end{itemize}


\subsection{Program verification according to Floyd}

If the following properties are given, then an algorithm is be correct. 

\begin{itemize}
    \item Partial correctness: if an answer is returned, it will be correct.
    \item termination: The algorithm eventually reaches a halt - meaning that an answer is always returned eventually. 
\end{itemize}

There is a simple method of proving an algorithm correct (by Floyd ):
\begin{itemize}
    \item Proving 'partial correctness' with invariant principle: there is some loop invariant that basically says: 'At this iteration the state is a little better than before'. For example, in a sorting problem, at the i'th iteration the subarray \inlinecode{data[0:i]} might already be sorted. 
    \item Proving 'termination' with well ordering principle
\end{itemize}

Note that there might be correct algorithms that don't have these properties. For example, some algorithms might eventually reach a desired state without ever having an ever-imporving loop invariant. Also, there are other ways to prove an algorithm correct than the Floyd-method. However, in many real-world examples, this is the easiest approach to formaly proving an algorithm correct. 

\subsubsection{ Proving 'partial correctness' with invariant principle }

\subsubsection{ Proving 'termination' with well ordering principle }



\subsection{Notation}

An algorithm that takes $f(n)$ cycles is said to take \BigTheta{g(n)} time, iff: 

$$ f \in \BigTheta{g(n)} $$ 
where 
$$ \BigTheta{g(n)} = \{ f(n) | \thereis c_1, c_2, n_0 : \forall n \geq n_0:  0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \} $$

Similarly, the set \BigOh{g(n)} is defined as: 
$$ \BigOh{g(n)} = \{ f(n) | \thereis c_2, n_0 : \forall n \geq n_0:  0 \leq  f(n) \leq c_2 g(n) \} $$

That is, \BigOh{} only has an upper bound, but no lower bound.


\subsection{Order of basic operations}

In the integer ram model of computation, addition, multiplication, remainder and bitwise operations all take O(1).



\subsection{Optimization}

\begin{itemize}
  \item \emph{Linear programming}: Minimizing $f$ (linear) subject to constraints (linear, inequalities)
  \item \emph{Lagrange mulitpliers}: Minimizing $f$ (non-linear) subject to constraints (non-linear, equalities)
  \item \emph{Non-linear programming}: Minimizing $f$ (non-linear) subject to constraints (non-linear, inequalities)
  \begin{itemize}
    \item $f$ and constraints are differentiable: use KKT (a generalization of Lagrange)
    \item $f$ and constraints are convex: use convex optimization
    \item All other cases: use some form of gradient descent (simulated annealing, MCMC, ...)
  \end{itemize}
\end{itemize}



\subsection{Memoization: Remembering the state in loops}
Sometimes, operations have to be repeatedly executed in a loop. These operations may take \BigOh{g} time by themselves, but only \BigOh{1} time if the result of the previous execution is already known. 

In such a case it makes sense to keep track of the previous result. StateMachines are a natural vehicle to store such information. 

Lets assume for now that modulo was not constant time. Then we could do the infamous FizzBuzz problem like this in linear time: 

\begin{lstlisting}[language=python]
class SM:
    def __init__(self, nr):
        self.nr = nr
        self.nToGo = nr

    def feed(self):
        self.nToGo -= 1
        if self.nToGo < 0:
            self.nToGo = self.nr - 1

    def isMult(self):
        return self.nToGo == 0



threeM = SM(3)
fiveM = SM(5)
for i in range(1, 16):
    threeM.feed()
    fiveM.feed()
    if threeM.isMult() and fiveM.isMult():
        print("{} --> FizzBuzz".format(i))
    elif threeM.isMult():
        print("{} --> Fizz".format(i))
    elif fiveM.isMult():
        print("{} --> Buzz".format(i))
    else:
        print(i)
\end{lstlisting}

\subsection{Using sub problems: Dynamic programming}
Solve a multi-step-problem by solving its sub-problems\footnote{Note that the optimal solution to a multi-step-problems is \emph{not always} equal to the sum of the optimal solutions of its sub-problems. Those weird problems are said to \emph{not have optimal substructure}. Example: longest path problem.}. 
Such problems usually use recursion and memoization.

\paragraph{Example: Towers of Hanoi}: You have three rods $A$, $B$ and $C$. Move a tower of (say) 4 disks from $A$ to $C$ using the rules:
\begin{itemize}
  \item move one disk at a time
  \item no larger disk may be placed on a smaller disk
\end{itemize}

Solution:
\begin{lstlisting}[language=python]
def move([4, 3, 2, 1], from: 'A', to: 'C', via: 'B'):
  return 
      move([3, 2, 1], from: 'A', to: 'B', via: 'C')
    + move([4],       from: 'A', to: 'C', via: 'B')
    + move([3, 2, 1], from: 'B', to: 'C', via: 'A')
\end{lstlisting}

The hardest part in dynamic programming is usually finding the right mental model to approach a problem. Often, there are at least two ways to look at a problem, and the less obvious one is the one that is actually useful.
Here's a list of ways to think of problems that was useful to me in the past:
\begin{itemize}
  \item \textbf{decision-tree} - example: create all subsets of a set
  \item \textbf{graph} - example: count ways to climb n stairs by using 1, 2, or 3 stairs at a time
  \item \textbf{binary search} - example: find a magic index (\inlinecode{i = a[i]}) in an ordered array. (Binary search: go to middle, then go to middle of left half, then go to middle of right half, ...)
\end{itemize}


\subsection{Avoiding the need for dynamic programming: Solving recurrences to closed form}
Sometimes we don't have to do a recursive solution at all. Many recursive problems can be reshaped into a explicit form (the so called closed form). Think of the Fibonacci-sequence: 

$$ fib(n) = fib(n-1) + fib(n-2) = \frac{(1 + \sqrt{5})^n - (1 - \sqrt{5})^n }{2^n \sqrt{5}} $$

This section will exemplify some methods for finding the closed form expression.

\subsubsection{Solving linear recurrences}

A homogeneous linear recurrence is one of the following form: 

$$ f(n) = a_1 f(n-1) + a_2 f(n-2) + ... + a_d f(n-d)$$

We can solve it as follows: 

\begin{enumerate}
    \item Assume $f(n) = x^n$. We now try to find an expression for $x$.
    \item Divide both sides in the hlr by $x^{n-d}$, leaving a (hyper-)quadratic equation. 
    \item Every root of the quadratic equation is a \textbf{homogeneous solution}. Also, if a root $r$ occures $v$ times, $r^n, nr^{n-1}, n^2r^n ..., n^{v-1}r^n$ are also solutions.
    \item Also, every linear combination of the above is also a solution. So, a solution might in general have a form like $a r_1^n + b r_2^n + ...$.
    \item Finally, choose $a, b, ...$ such that they fulfill the boundary conditions (in Fibonacci those would be $f(0) = f(1) = 1$). This is the \textbf{concrete solution}.
\end{enumerate}

We can extend the above schema to also solve (nonhomogeneous) linear recurrences: 
$$ f(n) = a_1 f(n-1) + a_2 f(n-2) + ... + a_d f(n-d) + g(n)$$

\begin{enumerate}
    \item Ignore $g(n)$, find the homogeneous solution from step 4 above.
    \item Find a \textbf{particular solution} to the equation including $g(n)$.
    \item homogeneous solution + particular solution = \textbf{general solution}
    \item Now for the general solution, again plug in the boundary conditions as in step 5 above.
\end{enumerate}


\section{Important algorithms}

\subsection{Merge-sort}

The function merge takes two already sorted lists and merges them into one sorted list.

\begin{lstlisting}[language=python]
def merge(a, b):
    c = []
    ia = ib = ic = 0
    while len(c) < len(a) + len(b):
        if a[ia] <= b[ib]:
            c[ic] = a[ia]
            ia++
        else:
            c[ic] = b[ib]
            ib++
        ic++
    return c

def sort(arr):
  l = len(arr)
  if l == 1:
    return arr
  sorted1 = sort(arr[:l/2])
  sorted2 = sort(arr[l/2:])
  return merge(sorted1, sorted2)
\end{lstlisting}

\invariant{statement}{sometext}{sometext}{sometext}

The \textbf{runtime} of the algorithm is $\BigOh{n \log n}$.

\subsection{Insertion-sort}

The idea is to go through a list from left to right. Look at all entries left of your current position. Move your current value back down the left list until it is sorted in. Go one step further right.
\begin{lstlisting}[language=python]
  def insertionSort(arr):
    # Traverse through 1 to len(arr)
    for i in range(1, len(arr)):
        pivot = arr[i]
        # Move elements of arr[0..i-1], that are
        # greater than pivot, to one position ahead
        # of their current position
        j = i-1
        while j >=0 and pivot < arr[j] :
                arr[j+1] = arr[j]
                j -= 1
        arr[j+1] = pivot
\end{lstlisting}

The \textbf{runtime} of the algorithm is $\BigOh{n^2}$ in the worst case, but $\BigOh{n}$ for almost sorted lists. That makes insertion sort great if you have to apply sort over and over again.


\subsection{Matrix inverse}
Gauss Jordan

\subsection{Polynomials}
In this section, we will deal with the polynomials $P_A(x) = 1x^3 + 2x^2 + 3x + 4$. Working with polynomials turns out to be quite important in everyday practice - especially when you're working with generating functions.

\subsubsection{Coefficient-representation of polynomals}
The most common representation of this polynomial is its coefficient representation $[4, 3, 2, 1]$. This representation is very convenient for fast evaluation at any $x$ using Horner's method: 

\begin{lstlisting}[language=c]
#include "stdio.h"
#include "stdlib.h"
#include "math.h"

/**
 * Coefficient representation of polynomals
 */

typedef struct PolynomalC {
  int length;
  int* coefficients;
} PolynomalC;

PolynomalC* polyc_create(int l, int* coeffs) {
  PolynomalC* p = malloc(sizeof(PolynomalC));
  p->length = l;
  p->coefficients = malloc(l*sizeof(int));
  for(int i = 0; i<l; i++){
    p->coefficients[i] = coeffs[i];
  }
  return p;
}

void polyc_delete(PolynomalC* p) {
  free(p->coefficients);
  free(p);
}

int polyc_evalAt(PolynomalC* p, int x) {
  int sum = 0;
  for(int i = p->length; i > 0; i--){
    sum = sum * x + p->coefficients[i-1];
  }
  return sum;
}

PolynomalC* polyc_add(PolynomalC* p1, PolynomalC* p2) {
  int l1 = p1->length;
  int l2 = p2->length;
  int l = max(l1, l2);
  int coeffs[l];
  for(int i = 0; i < l; i++){
    int v = 0;
    if(i < l1) v += p1->coefficients[i];
    if(i < l2) v += p2->coefficients[i];
    coeffs[i] = v;
  }
  PolynomalC* p3 = polyc_create(l, coeffs);
  return p3;
}

int main(void) {
  
  int coeffs1[4] = {4, 3, 2, 1};
  PolynomalC* p1 = polyc_create(4, coeffs1);
  
  int coeffs2[3] = {1, 2, 3};
  PolynomalC* p2 = polyc_create(3, coeffs2);
  
  PolynomalC* p3 = polyc_add(p1, p2);
  
  int r = polyc_evalAt(p3, 2);
  printf("Result: %d\n", r);
  
  polyc_delete(p1);
  polyc_delete(p2);
  polyc_delete(p3);
  
  return 0;
}
\end{lstlisting}

\subsubsection{Point-value-representation of polynomals}
Another way of representing this polynomal is the point-value representation $[(0,4), (1, 10), (2, 26), (3,58)]$. This representation is very convenient for fast evaluation of polynomal multiplication (assuming both polys are evaluated at exactly the same points): 

\begin{lstlisting}[language=c]
#include "stdio.h"
#include "stdlib.h"
#include "math.h"

/**
 * Point/Value representation of polynomals
 */

typedef struct PolynomalV {
  int length;
  int* points;
  int* values;
} PolynomalV;

PolynomalV* polyv_create(int l, int* pts, int* vls) {
  PolynomalV* p = malloc(sizeof(PolynomalV));
  p->length = l;
  p->points = malloc(sizeof(int)*l);
  p->values = malloc(sizeof(int)*l);
  for(int i = 0; i < l; i++) {
    p->points[i] = pts[i];
    p->values[i] = vls[i];
  }
  return p;
}

void polyv_delete(PolynomalV* p) {
  free(p->points);
  free(p->values);
  free(p);
}

PolynomalV* polyv_mult(PolynomalV* p1, PolynomalV* p2) {
  int l = p1->length;
  int vals[l];
  for(int i = 0; i < l; i++) {
    vals[i] = p1->values[i] * p2->values[i];
  }
  PolynomalV* p = polyv_create(l, p1->points, vals);
  return p;
}

int main(void) {
  
  int points[4] = {0, 1, 2, 3};
  
  int vals1[4] = {4, 10, 26, 58};  // [4, 3, 2, 1]
  PolynomalV* p1 = polyv_create(4, points, vals1);
  
  int vals2[4] = {1, 10, 49, 142}; // [1, 2, 3, 4]
  PolynomalV* p2 = polyv_create(3, points, vals2);
  
  PolynomalV* p3 = polyv_mult(p1, p2);
  
  polyv_delete(p1);
  polyv_delete(p2);
  polyv_delete(p3);
  
  return 0;
}
\end{lstlisting}

\subsubsection{Naive transformation from coefficient to point-value and vice-versa}
\begin{lstlisting}[language=c]

// O(n^2)
 PolynomalV* poly_coeffToPv(PolynomalC* pc, int* points){
   int l = pc->length;
   int vals[l];
   for(int i = 0; i < l; i++) { // n operations
     vals[i] = polyc_evalAt(pc, points[i]); // O(n)
   }
   PolynomalV* pv = polyv_create(l, points, vals);
   return pv;
 }

// O(n^3)
PolynomalC* poly_PvToCoeff(PolynomalV* p){
  int l = p->length;
  int points[l] = p->points;
  int values[l] = p->values;
  int** data = [[1, x1, x1^2, ...]
                [1, x2, x2^2, ...]
                [1, x3, x3^2, ...]];
  Matrix* m = mtrx_create(l, l, data);
  Matrix* mi = mtrx_inverse(m);   // Gauss Jordan: O(n^3)
  int coeffs[l] = mtrx_mult(mi, values);
  PolynomalC* pc = polyc_create(l, coeffs);
  return pc;
}


int main(void) {
  
  int coeffs[4] = {4, 3, 2, 1};
  PolynomalC* pc = polyc_create(4, coeffs);
  int points[4] = {0, 1, 2, 3};
  PolynomalV* pv = poly_coeffToPv(pc, points);
  PolynomalC* px = poly_PvToCoeff(pv);
  
  polyc_delete(pc);
  polyv_delete(pv);
  polyc_delete(px);
  
  return 0;
}
\end{lstlisting}

\subsubsection{Fast transformation}
Up to now we have used the points $0, 1, 2, 3$ for our evaluation. However, there is no reason why we should have chosen these specific points - any other points of evaluation are just as good. It turns out that if we use the 4 roots of 1 as points of evaluation, we can cut down on computation costs.

In general, the $k$th root of 1 is $e^{2 \pi i \frac{k}{n} }$. So let's do the evaluation at $(1^{1/4})_1, (1^{1/4})_2, (1^{1/4})_3, (1^{1/4})_4 = 1, i, -1, -i$.



\subsection{Stream algorithms}
Stream algorithms handle the case where a function is not applied once for a single input, but continuously on a neverending stream of inputs. This creates some constraints: 
\begin{itemize}
  \item The algorithm must not take longer than the feed-rate of the stream
  \item The algorithm must use only a constant ammount of storage (because otherwise at some point the disc will be full)
\end{itemize}
\href{https://www.americanscientist.org/article/the-britney-spears-problem}{Here} is a good introduction to the toppic. 
Side note: a popular software to provide a stream to (multiple) subscriber(s) would be \href{https://kafka.apache.org/}{Apache kafka}.