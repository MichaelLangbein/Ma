\section{Tensorflow}

\subsection{Low-level API}

Working with tf always entails a two-step process. First, define a graph, and second, execute it. 

\begin{lstlisting}[language=python]
import tensorflow as tf

# Step 1: define a graph
a = tf.Variable(initial_value=2) # Tensors are immutable by default. We must wrap them in a Variable to allow them to change
b = tf.Variable(initial_value=3)
c = a * b

# Step 2: execute graph
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    result = sess.run(c)
    print(result)

\end{lstlisting}

\emph{Running} a tensor means evaluating it - in the case of \inlinecode{c}, that means calculating \inlinecode{a * b}.
There are two things that can be run by a session: tensors (to be evaluated) and operations (to be executed). 

Tf is declarative: \inlinecode{c = a*b} does not return the result of the multiplication of a and b, but a tensor that is still waiting to be evaluated. 
This can make debuggung pretty hard, though. For debugging, we'd like tf to behave more imperatively, where every expression is immediately evaluated. 
We can acchieve this by using an interactive session, where every tensor can be evaluated immediately by calling \inlinecode{eval()}:
\begin{lstlisting}[language=python]
    sess = tf.InteractiveSession()
    c.eval()
\end{lstlisting}

Optimizers are a very common example of operations, so we show a very simple example of their usage.

\begin{lstlisting}[language=python]
    import tensorflow as tf
    import numpy as np
    import matplotlib.pyplot as plt
    
    
    N = 100
    a = 0.2
    b = 5
    xs_training = np.random.random(N) * 100
    ys_training = a*xs_training + b + np.random.random(N)
    xs_validation = np.random.random(N) * 100
    ys_validation = a*xs_validation + b + np.random.random(N)
    
    
    a_tf = tf.Variable(0.3421)
    b_tf = tf.Variable(0.41)
    xs_tf = tf.placeholder(tf.float32, shape=[N])
    ys_tf = tf.placeholder(tf.float32, shape=[N])
    model_tf = a_tf * xs_tf + b_tf
    loss_tf = tf.reduce_sum((model_tf - ys_tf)**2)
    optimizer = tf.train.AdamOptimizer(learning_rate=0.1)
    training_operation = optimizer.minimize(loss_tf, var_list=[a_tf, b_tf])
    
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
    
        for i in range(1000):
            sess.run(
                training_operation, 
                {xs_tf: xs_training, ys_tf: ys_training}
            )
    
        prediction = sess.run(
            model_tf, 
            {xs_tf: xs_validation}
        )
    
    
    plt.scatter(xs_validation, prediction)
    plt.scatter(xs_validation, ys_validation)
    plt.show()

\end{lstlisting}


\subsection{Even lower level, and understsanding TF internals}

Tensorflow is an automatic differentiation engine. That is not quite the same as a symbolic differentiation engine. The difference becomes appparent when you create a custom operation to add to TF. To do this, you need to: 

\begin{itemize}
    \item register a new operation in c++
        \begin{itemize}
            \item operation-name
            \item inputs including their shapes
        \end{itemize}
    \item implement the operation in c++
        \begin{itemize}
            \item operation
            \item gradient of operation for all inputs
        \end{itemize}
    \item create a python wrapper for the public python-tf-api (not really neccessary, because tf creates a default wrapper automatically)
\end{itemize}

So contrary to a symbolic engine, you have to explain to tf by hand how to calculate a gradient. 

\subsection{High-level API}

When using keras, you usually don't see the low-level api at all - no tensors, no sessions, no \inlinecode{run}.

\begin{lstlisting}[language=python]

import numpy as np
import tensorflow as tf
import tensorflow.keras as k
import matplotlib.pyplot as pyplot


# 1. Data
N = 100
inputs = []
outputs = []
for n in range(N):
    i1 = np.random.randint(0,2)
    i2 = np.random.randint(0,2)
    o = 1 * ((i1 + i2) == 1)
    inputs.append([i1, i2])
    outputs.append([o])
inputs_np = np.array(inputs)
outputs_np = np.array(outputs)


# 2. Model
model = k.Sequential(layers=[
    k.layers.Dense(2, activation=k.activations.sigmoid),
    k.layers.Dense(1, activation=k.activations.sigmoid)
])

model.compile(
    optimizer=tf.train.AdamOptimizer(learning_rate=0.1),
    loss=tf.losses.mean_squared_error
)

# 3. Running

model.fit(x=inputs_np, y=outputs_np, epochs=1000)

result = model.predict(np.array([[0,0], [0,1], [1,0], [1,1]]))

print(result)



\end{lstlisting}


\subsection{Toppics}

\subsubsection{Convolutional nets}
\begin{lstlisting}[language=python]
    import tensorflow as tf
    import tensorflow.keras as k
    import matplotlib.pyplot as plt
    import numpy as np
    import dummydata as dd
    import utils as u
    
    
    # 1. Data
    imageWidth = imageHeight = 80
    batchSize  = 10
    timeSteps = 10
    #dd.createDummyDataset(batchSize, timeSteps, imageWidth, imageHeight)
    data_in, data_out = dd.loadDummyDataset()
    
    
    
    # 2. Net
    model = k.Sequential([
        k.layers.Conv3D(5, (2,2,2),     name="conv3d_1",    input_shape=(timeSteps, imageWidth, imageHeight, 1)),
        k.layers.MaxPool3D(             name="maxpool_1"),
        k.layers.Conv3D(5, (2,2,2),     name="conv3d_2"),
        k.layers.MaxPool3D(             name="maxpool_2"),
        k.layers.Flatten(),
        k.layers.Dense(30,              name="dense_1"),
        k.layers.Dense(10,              name="dense_2"),
        k.layers.Dense(1,               name="dense_3")
    ])
    
    model.compile(
        optimizer=k.optimizers.Adam(),
        loss=k.losses.mean_squared_error
    )
    
    model.fit(x=data_in, y=data_out, batch_size=batchSize, epochs=10)
    
    model.predict(np.array([data_in[0]]))
    
\end{lstlisting}


\subsubsection{LSTM}
Lstm's can be tricky in that they need input in a very specific format.
\begin{lstlisting}[language=python]
    import numpy as np
    import tensorflow as tf
    import tensorflow.keras as k
    
    
    
    # 1. Data: sine-curve
    nr_samples = 10
    nr_timesteps = 4
    nr_pixels = 1
    inpt = np.zeros([nr_samples, nr_timesteps, nr_pixels])
    outpt = np.zeros([nr_samples, nr_pixels])
    for sample in range(nr_samples):
        for timestep in range(nr_timesteps):
            for pixel in range(nr_pixels):
                inpt[sample, timestep, pixel] = (np.sin(timestep * 0.1 + sample) + 1.0) / 2.0
        for pixel in range(nr_pixels):
            outpt[sample, pixel] = (np.sin((nr_timesteps + 1) * 0.1 + sample) + 1.0) / 2.0
        
    
    
    # 2. Network: simple lstm
    model = k.Sequential([
        k.layers.LSTM(1) # 1 = output dimension. for every timeseries we pass in, we want the one next value
    ])
    
    model.compile(
        optimizer=tf.train.AdamOptimizer(learning_rate=0.1),
        loss=tf.losses.mean_squared_error
    )
    
    model.fit(x=inpt, y=outpt, epochs=500)
    
    model.predict(
        np.array([[
           [0.5       ],
           [0.54991671],
           [0.59933467],
           [0.6477601 ]
        ]])
    )
    
\end{lstlisting}
