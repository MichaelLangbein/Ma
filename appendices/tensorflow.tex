\section{Tensorflow}

\subsection{Low-level API}

Working with tf always entails a two-step process. First, define a graph, and second, execute it. 

\begin{lstlisting}[language=python]
import tensorflow as tf

# Step 1: define a graph
a = tf.Variable(initial_value=2) # Tensors are immutable by default. We must wrap them in a Variable to allow them to change
b = tf.Variable(initial_value=3)
c = a * b

# Step 2: execute graph
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    result = sess.run(c)
    print(result)

\end{lstlisting}

\emph{Running} a tensor means evaluating it - in the case of \inlinecode{c}, that means calculating \inlinecode{a * b}.
There are two things that can be run by a session: tensors (to be evaluated) and operations (to be executed). 

Tf is declarative: \inlinecode{c = a*b} does not return the result of the multiplication of a and b, but a tensor that is still waiting to be evaluated. 
This can make debuggung pretty hard, though. For debugging, we'd like tf to behave more imperatively, where every expression is immediately evaluated. 
We can acchieve this by using an interactive session, where every tensor can be evaluated immediately by calling \inlinecode{eval()}:
\begin{lstlisting}[language=python]
    sess = tf.InteractiveSession()
    c.eval()
\end{lstlisting}

Optimizers are a very common example of operations, so we show a very simple example of their usage.

\begin{lstlisting}[language=python]
    import tensorflow as tf
    import numpy as np
    import matplotlib.pyplot as plt
    
    
    N = 100
    a = 0.2
    b = 5
    xs_training = np.random.random(N) * 100
    ys_training = a*xs_training + b + np.random.random(N)
    xs_validation = np.random.random(N) * 100
    ys_validation = a*xs_validation + b + np.random.random(N)
    
    
    a_tf = tf.Variable(0.3421)
    b_tf = tf.Variable(0.41)
    xs_tf = tf.placeholder(tf.float32, shape=[N])
    ys_tf = tf.placeholder(tf.float32, shape=[N])
    model_tf = a_tf * xs_tf + b_tf
    loss_tf = tf.reduce_sum((model_tf - ys_tf)**2)
    optimizer = tf.train.AdamOptimizer(learning_rate=0.1)
    training_operation = optimizer.minimize(loss_tf, var_list=[a_tf, b_tf])
    
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
    
        for i in range(1000):
            sess.run(
                training_operation, 
                {xs_tf: xs_training, ys_tf: ys_training}
            )
    
        prediction = sess.run(
            model_tf, 
            {xs_tf: xs_validation}
        )
    
    
    plt.scatter(xs_validation, prediction)
    plt.scatter(xs_validation, ys_validation)
    plt.show()

\end{lstlisting}
