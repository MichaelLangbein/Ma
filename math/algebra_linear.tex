\section{Linear algebra}

In the previous section on general algebra we dealt with vector spaces (subspaces, linear independence, bases) and inner product spaces (norms, orthogonality). Here, we'll take a step back and only consider vector spaces, not requireing them to also be equipped with an inner product. 


\subsection{Matrix properties}

\paragraph{Matrices form a vector space} This can be easily proven.

\begin{proof}[The set of all $n \times m$ matrices $M$ is a vector space.] For this, we just need to go through the seven properties from the previous chapter. 

    \subprf{}{scalar multiplication properties hold}{
    
        \subprf{}{$M$ is closed under scalar multiplication}{}
        \subprf{}{$a(b \mtrx{m}) = (ab)\mtrx{m}$}{}
        
    }
    
    \subprf{}{vector addition properties hold}{
    
    }
    
    \subprf{}{vector addition and scalar multiplication are distributive}{
    
    }
\end{proof}


Here is a little exercise to get you warmed up. 

\begin{proof}[If $\mtrx{A}\vec{x}$ is zero for any $\vec{x}$, then $\mtrx{A}$ must be the zero matrix.]

    \subprf{}{$\forall \vec{x} : \mtrx{A}\vec{x} = \vec{0} \then \mtrx{A} = \mtrx{0} $}{
        \subprf{Let $\vec{x}_0$ be chosen. Suppose $\mtrx{A}\vec{x}_0 = \vec{0}$.}{$\mtrx{A} = \mtrx{0} $}{
        
        
        }
    }

\end{proof}


Here is a method to prove that two matrices are identical. 

\begin{proof}
    \subprf{}{$\forall \vec{x}: \mtrx{A}\vec{x} = \mtrx{B}\vec{x} \then \mtrx{A} = \mtrx{B}$}{
        \subprf{Let $\vec{x}$ be chosen. Suppose $\mtrx{A}\vec{x} = \mtrx{B}\vec{x}$}{$\mtrx{A} = \mtrx{B}$}{
            
        }
    }
\end{proof}


\subsection{Spaces}

\begin{definition}[Nullspace]:
     $0_A$ is the nullspace of $A$. It is defined as $0_A = \{ x | Ax = 0 \}$
\end{definition}

\begin{definition}[Columnspace]:
     $C_A$ is the collumnspace of $A$. It is defined as $C_A = \{ y | Ax = y \}$
\end{definition}

\begin{definition}[Rowspace]:
     $R_A$ is the rowspace of $A$. It is defined as $R_A = \{ y | A^Tx = y \}$
\end{definition}

\begin{definition}[Rank]:
     $r_A$ is the rank of $A$. It is defined as the dimension of $C_A$
\end{definition}


\begin{proof}
$r_A = dim_{C_A} = dim_{R_A}$
\end{proof}

\begin{proof}
$A:basis_{C_A} \then r_A = n$
\end{proof}


\subsection{Inverses}
If $A$ has dimensions $n \times n$, then the inverse $A^{-1}$ such that
$$ A A^{-1} = A^{-1} A = I $$
exists iff $det_A \neq 0$.

\paragraph{Nonsqare} matrices do not have an inverse, but they might have a right- or left-inverse.
Consider $C = A A^T$. This is a square matrix, so it might have an inverse:
$$ A A^T (A A^T)^{-1} = I $$
Calling $A^T (A A^T)^{-1} = A^{RI}$ the right inverse:
$$ A A^{RI} = I $$
For $C^{-1}$ to exist, we require $C$ to be full-rank, which means that $A$ must be full row rank. This  requires $r \leq c$, in other words, $A$ being a broad matrix.

\begin{proof}
If $A^{RI}$ exists, then $A^{LI}$ does not
\end{proof}




\subsection{Change of basis}\label{changeOfBasis}

Let $V$ be a vector space. Let $0$ be the canonical basis for that vector space. Let $A = \{\vec{a}_1, ..., \vec{a}_N \}$ and $B = \{\vec{b}_1, ..., \vec{b}_N\}$ be two other basis for that vectorspace. Let $\mtrx{A}$ be the matrix $[\vec{a}_1  ...  \vec{a}_N]$ and $\mtrx{B} = [\vec{b}_1 ... \vec{b}_N]$

Every vector $\vec{v}$ can be expressed as a linear sum of the basisvectors in $A$, that is $\vec{v} = \sum_n \alpha_n \vec{a}_n$. That same thing in matrix notation: $\vec{v} = \mtrx{A}(\vec{v})_A$, where $(\vec{v})_A$ is the coordinates $\alpha$ of $\vec{v}$ with respect to the basis $A$. Correspondingly, for $B$ we have $\vec{v} = \sum_n \beta_n \vec{b}_n = \mtrx{B}(\vec{v})_B$.

Note that within $A$ and $B$, we express the basevectors with respect to the canonical basis $0$, that is, we should really write $\mtrx{A} = [(\vec{a}_1)_0  ...  (\vec{a}_N)_0]$. Note also that $[(\vec{a}_1)_A  ...  (\vec{a}_N)_A] = \mtrx{I}$, the identity matrix. 

We can use this to obtain a simple formula for the change of basis. 
$$ \vec{v} = \mtrx{A} (\vec{v})_A $$
$$ \vec{v} = \mtrx{B} (\vec{v})_B $$
$$ (\vec{v})_B = \mtrx{B}^{-1} \mtrx{A} (\vec{v})_A $$

But inverses are notoriously hard to calculate. Fortunately, there is another approach. Call $\mtrx{T}_{BA} = [(\vec{a}_1)_B ... (\vec{a}_N)_B]$ the \emph{transition matrix}. We can prove that $\mtrx{B}^{-1} \mtrx{A} = \mtrx{T}_{BA}$:

\begin{equation}
\begin{aligned}
\mtrx{B}^{-1} \mtrx{A} (\vec{v})_A &= \mtrx{T}_{BA}  (\vec{v})_A \\
                                   &= \sum_n (v_n)_A (\vec{a}_n)_B \\
                                   &= \sum_n (v_n)_A \mtrx{B}^{-1} \mtrx{A} (\vec{a}_n)_A \\
                                   &= \mtrx{B}^{-1} \mtrx{A} \sum_n (v_n)_A (\vec{a}_n)_A \\
                                   &= \mtrx{B}^{-1} \mtrx{A} \text{  } \mtrx{I} (\vec{v})_A \\
                    (\vec{v})_A &=  (\vec{v})_A         
\end{aligned}
\end{equation}

Using $\mtrx{T}_{BA} = \mtrx{B}^{-1}\mtrx{A}$, a lot of statements are trivial to prove:
\begin{itemize}
    \item $\mtrx{T}_{BA} = \mtrx{T}_{AB}^{-1}$
    \item $\mtrx{T}_{CA} = \mtrx{T}_{CB} \mtrx{T}_{BA}$
\end{itemize}


\subsection{Linear transformations}

\begin{definition}
Let $U$ and $V$ be two vector spaces and $f:U \to V$. Then $f$ is a \emph{linear transform} if
\begin{itemize}
    \item $f$ preserves scalar multiplication: $f(\alpha \vec{u}) = \alpha f(\vec{u})$
    \item $f$ preserves vector addition: $f(\vec{u}_1 + \vec{u}_2) = f(\vec{u}_1) + f(\vec{u}_2)$
\end{itemize}
\end{definition}

There are a bunch of properties to linear transformations that can be useful to us. 

\begin{proof}[There is a unique linear transform from the basis of $U$ to any set of vectors in $V$ that we want.] In other words, any linear transform $f$ is completely deterimed by the matrix $[ f(\vec{b}_1) ... f(\vec{b}_N) ] = [\vec{v}_1 ... \vec{v}_N]$. That means if we dont know the transform, but we do know the results of the transform on a basis, then we can reconstruct the transform with certainty.  \\

    \subprf{Let $B = \{ \vec{b}_1, ..., \vec{b}_N \}$ be a basis for $U$. Let $\{ \vec{v}_1, ..., \vec{v}_N \}$ be any vectors in $V$ that we may chose. }
    { there is a unique function $f:U \to V$ such that $f(\vec{b}_i) = \vec{v}_i$ }{
    
        \subprf{Try $f(\vec{x}) = \mtrx{V} (\vec{x})_B$}
        {$f(\vec{b}_i) = \vec{v}_i$ and $f$ is a linear transform.}{
        
            \subprf{Part 1: }
            {$f(\vec{b}_i) = \vec{v}_i$}{
                
                $ f(\vec{b}_i) = \mtrx{V} (\vec{b}_i)_B = \mtrx{V} \vec{e}_i = \vec{v}_i $
                
            }
            
            \subprf{Part 2: }{$f$ is unique}{
                
                We could not have obtained any other form of $f$ than $f(\vec{x}) = \mtrx{V} (\vec{x})_B$. This is because for \emph{any} linear transform from $U \to V$ we have: 
                
                $ f(\vec{x}) = f(\mtrx{B}(\vec{x})_B) = f(\sum_n (x_n)_B \vec{b}_n) = \sum_n (x_n)_B f(\vec{b}_n)  $
                
                Using the result from part 1, this cannot be any other function than: 
                
                $ \sum_n (x_n)_B f(\vec{b}_n) = \sum_n (x_n)_B \vec{v}_n = \mtrx{V} (\vec{x})_B $ 
                
            }
            
            \subprf{Part 3: }
            {$f$ is a linear tansform}{
            
            }
        }
    }
\end{proof}

A whole bunch of other properties are now easily proved. Let $f$ and $g$ be linear transforms from $U$ to $V$. The following are also linear transforms: 
\begin{itemize}
    \item $\alpha f$
    \item $f + g$
    \item $f^{-1}$ ( if it exists )
    \item $fg$ ( here $g: V \to W$ )
\end{itemize}

Let $f: U \to V$ be a linear transform. Then the following are equivalent: 
\begin{itemize}
    \item If $f(\vec{u}) = \vec{0}$, then $\vec{u} = \vec{0}$
    \item $f$ is one-to-one
    \item $f$ maps linearly independent vectors to linearly independent vectors. 
\end{itemize}


Prove that a transform can be split up into mulitple transforms on the basis vectors. 
As an examle, consider the case of a rotation. A diagonal rotation can be reproduced by a rotation first around one, then around another axis. 


\paragraph{A linear transformation can also be a change of basis} when it is on a vectorspace and invertible.


\subsection{Systems of linear equations}

