\section{Artificial intelligence}

\subsection{Categorisation}

\subsubsection{Neural networks}

\paragraph{Backpropagation networks}

There are two ways in which we can derive the backpropagation-algorithm. One is purely analytical, the other bayesian. We'll demonstrate both. 

\paragraph{The analytical way of deriving the backpropagaion algorithm} consist of just a few steps. 
A few definitions: 

\begin{itemize}
	\item A layers outputvector $\vec{y}^l$ is obtained by the activation function $\vec{y}^l = f(\vec{x}^l) $
	\item The layers inputvector is obtained as a weighted sum of previous outputs: $\vec{x}^l = \mtrx{W}^l \vec{y}^{l-1} $. We cn express a single $x_t^l = \sum_f W_{t,f}^l y_f^{l-1}$
	\item We strive to minimize the error-function. Assuming only one single training item we get $e = \frac{1}{2} \sum_t (\vec{y}^*_t - \vec{y}^L_t)^2 $
\end{itemize}

Let's first consider only the top layer. 

$$  \partDiff{e}{x_{t_0}^L} = \frac{1}{2} \sum_t \partDiff{}{x_{t_0}^L} (\vec{y}^*_t - \vec{y}^L_t)^2 $$
$$ = (y_{t_0}^* - y_{t_0}^L) f'(x_{t_0}) $$

Or, in vector form: 

$$ \partDiff{e}{\vec{x}^L} = (\vec{y}^* - \vec{y}^L)^T \pointwise f'(\vec{x}^L)  $$


That part was easy. But how do we obtain the same differential for \emph{any} layer $l$?

$$ \partDiff{e}{x_{f_0}^l} = \sum_t \partDiff{e}{x_t^{l+1}} \partDiff{x_t^{l+1}}{x_{f_0}^l}  $$
$$                         = \sum_t \partDiff{e}{x_t^{l+1}} \partDiff{}{x_{f_0}^l} ( \sum_f W_{t,f}^{l+1} y_f^l ) $$
$$                         = \sum_t \partDiff{e}{x_t^{l+1}} W_{t,f_0}^{l+1} f'(x_{f_0}^l)  $$

Or, in vector form: 

$$ \partDiff{e}{\vec{x}^l} = ( \partDiff{e}{\vec{x}^{l+1}} \mtrx{W}^{l+1} ) \pointwise f'(\vec{x}^l)  $$

The smart part here was to not derive $ \partDiff{e}{\vec{x}^l} $ by going through $\vec{y}^L$, $\vec{x}^L$, $\mtrx{W}^L$, $\vec{y}^{L-1}$, $\vec{x}^{L-1}$, $\mtrx{W}^{L-1}$, ..., but by instead creating a recurrence relation by differentiating by $\vec{x}^{l+1}$.

Finally, we can obtain the gradient at our weights as: 

$$ \partDiff{e}{W_{t_0, f_0}^l} = \partDiff{e}{x_{t_0}^l} \partDiff{x_{t_0}^l}{W_{t_0, f_0}^l}   $$
$$                              = \partDiff{e}{x_{t_0}^l} \partDiff{}{W_{t_0, f_0}^l} ( \sum_f W_{t_0, f}^l y_f^{l-1} ) $$
$$                              = \partDiff{e}{x_{t_0}^l} y_{f_0}^{l-1} $$

Or, in vector form: 

$$ \partDiff{e}{\mtrx{W}^l} = {\partDiff{e}{\vec{x}^l}}^T  {\vec{y}^{l-1}}^T $$ 

So we should change the weigths by: 

$$ \Delta \mtrx{W}^l = - \alpha \partDiff{e}{\mtrx{W}^l} $$
$$ = - \alpha \partDiff{e}{\vec{x}^l} \vec{y}^{l-1} $$


It makes sense to reiterate the whole process in matrix-form. 

First, we  get $\delta^L$:
$$ \partDiff{e}{\vec{y}^L} = (\vec{y}^* - \vec{y}^L)^T := \delta^L $$ 

Then we go through the highest layer:
$$ \partDiff{e}{\vec{x}^L} = \delta^L \pointwise f'(\vec{x}^L)  $$
$$ \partDiff{e}{\mtrx{W}^L} = {\partDiff{e}{\vec{x}^L}}^T {\vec{y}^{L-1}}^T $$
$$ \delta^{L-1} = \partDiff{e}{\vec{x}^L} \mtrx{W}^L $$

Then we pass $\delta^{L-1}$ to the next layer. 
$$ \partDiff{e}{\vec{x}^l} = \delta^l \pointwise f'(\vec{x}^l)  $$
$$ \partDiff{e}{\mtrx{W}^l} = {\partDiff{e}{\vec{x}^l}}^T {\vec{y}^{l-1}}^T $$
$$ \delta^{l-1} = \partDiff{e}{\vec{x}^l} \mtrx{W}^l $$




\paragraph{Gaussian processes}

The bayesian derivation of the backpropagation algorithm gives rise to the interesting idea that backprop-networks may be generalized into ...

\paragraph{Convolutional networks}

These are the networks most commonly found employed in image-classification. Really, they are just a simplified version of our backpropagation-networks (they are even trained using an only slightly altered algorithm). Instead of connecting every node from layer $l$ to every node of layer $l+1$, they impose some restrictions on the connection-matrix:
\begin{itemize}
	\item Nodes are connected in a pyramid scheme. A node on layer $l+1$ is connected to 9 nodes directly beneath it. Instead of a $n_l \times n_{l+1}$ connection matrix, we thus have several $9 \times 1$ matrices.
	\item The connection-strengths of these  $9 \times 1$ matrices are all the same - so really there is only just one  $9 \times 1$ matrix. 
\end{itemize}
These restrictions are inspired by the physiology of the visual cortex. They have the nice effect that a network is trained much faster, since they massively reuce the ammount of weights that need to be learned. 

In practice, such networks have a few convolutional layers to reduce the dimension of the input-images, followed by a few conventional, fully connected layers that learn some logic based on the reduced images. 

We give the backpropagation-steps for convolutional layers and pooling layers. 

In convolutional layers, the forward step goes: 

$$ \vec{x}^l = \vec{y}^{l-1} \convol \vec{w}^l $$
$$ \vec{y}^l = f(\vec{x}^l) $$

Where the convolution is defined (in our simplified case) as:
$$ (\vec{y} \convol \vec{w})_n = \sum_{m=-1}^1 \vec{y}_{n+m} \vec{w}_m $$

Differentiating a convolution is unfamiliar, but not too hard: 
$$ \partDiff{(\vec{y} \convol \vec{w})}{\vec{w}} = \myarray{
	0   && y_0 && y_1 \\
	y_0 && y_1 && y_2 \\
	y_1 && y_2 && y_3 \\
	... \\
	y_{l-1} && y_l && 0 \\
} := tr(\vec{y}) $$
$$ \partDiff{(\vec{y} \convol \vec{w})}{\vec{y}} = \myarray{
	w_0    && w_1    && 0   && ... \\
	w_{-1} && w_0    && w_1 && ... \\
	0      && w_{-1} && w_0 && ... \\
	...    \\
	0      && ...    && w_{-1} && w_0 \\
} := br(\vec{w}) $$



Accordingly, the backwards step goes:

$$ \partDiff{e}{\vec{x}^l} = \delta^l \pointwise f'(\vec{x}^l) $$
$$ \partDiff{e}{\vec{w}^l} = \partDiff{e}{\vec{x}^l} tr(\vec{y}) = (\sum_{n=1}^l e'_{x_n} y_{n+1}, \sum_{n=0}^l e'_{x_n} y_n, \sum_{n=0}^{l-1} e'_{x_n} y_{n+1}) $$
$$ \delta^{l-1} = \partDiff{e}{\vec{x}^l} br(\vec{w}) = \partDiff{e}{\vec{x}^l} \convol \vec{w} $$

In pooling layers, the forward step goes: 

$$ x_t^l = \frac{1}{4} \sum_f y_{4t + f}^{l-1} $$
$$ y_t^l = x_t^l $$

And the  backwards step: 

$$ \partDiff{e}{\vec{x}^l} = \partDiff{e}{\vec{y}^l} \partDiff{\vec{y}^l}{\vec{x}^l}  = \delta^l  $$
$$ \partDiff{e}{\vec{w}^l} = 0 $$
$$ \delta^{l-1} = \frac{1}{4} \partDiff{e}{\vec{x}^l}  $$

\subsection{Computer vision}

\paragraph{Feature detection and pose estimation}

Say you want to locate the position of the nose in a portrait. 



\subsection{Symbolic AI}

Contrary to the before mentioned approaches, symbolic AI uses logical deduction instead of numerical processing to arrive at decissions. A good tutorial can be found here: \href{codeproject.com/Articles/179375/Man-Marriage-and-Machine-Adventures-in-Artificia}.

