\section{Artificial intelligence}

\subsection{Categorisation}

\subsubsection{Neural networks}

\paragraph{Backpropagation networks}

There are two ways in which we can derive the backpropagation-algorithm. One is purely analytical, the other bayesian. We'll demonstrate both. 

\paragraph{The analytical way of deriving the backpropagaion algorithm} consist of just a few steps. 
A few definitions: 

\begin{itemize}
	\item A layers outputvector $\vec{y}^l$ is obtained by the activation function $\vec{y}^l = f(\vec{x}^l) $
	\item The layers inputvector is obtained as a weighted sum of previous outputs: $\vec{x}^l = \mtrx{W}^l \vec{y}^{l-1} $. We cn express a single $x_t^l = \sum_f W_{t,f}^l y_f^{l-1}$
	\item We strive to minimize the error-function. Assuming only one single training item we get $e = \frac{1}{2} \sum_t (\vec{y}^*_t - \vec{y}^L_t)^2 $
\end{itemize}

Let's first consider only the top layer. 

$$  \partDiff{e}{x_{t_0}^L} = \frac{1}{2} \sum_t \partDiff{}{x_{t_0}^L} (\vec{y}^*_t - \vec{y}^L_t)^2 $$
$$ = (y_{t_0}^* - y_{t_0}^L) f'(x_{t_0}) $$

Or, in vector form: 

$$ \partDiff{e}{\vec{x}^L} = (\vec{y}^* - \vec{y}^L) \pointwise f'(\vec{x}^L)  $$


That part was easy. But how do we obtain the same differential for \emph{any} layer $l$?

$$ \partDiff{e}{x_{f_0}^l} = \sum_t \partDiff{e}{x_t^{l+1}} \partDiff{x_t^{l+1}}{x_{f_0}^l}  $$
$$                         = \sum_t \partDiff{e}{x_t^{l+1}} \partDiff{}{x_{f_0}^l} ( \sum_f W_{t,f}^{l+1} y_f^l ) $$
$$                         = \sum_t \partDiff{e}{x_t^{l+1}} W_{t,f_0}^{l+1} f'(x_{f_0}^l)  $$

Or, in vector form: 

$$ \partDiff{e}{\vec{x}^l} = ( \mtrx{W}^{l+1}^T \partDiff{e}{\vec{x}^{l+1}} ) \pointwise f'(\vec{x}^l)  $$

The smart part here was to not derive $ \partDiff{e}{\vec{x}^l} $ by going through $\vec{y}^L$, $\vec{x}^L$, $\mtrx{W}^L$, $\vec{y}^{L-1}$, $\vec{x}^{L-1}$, $\mtrx{W}^{L-1}$, ..., but by instead creating a recurrence relation by differentiating by $\vec{x}^{l+1}$.

Finally, we can obtain the gradient at our weights as: 

$$ \partDiff{e}{W_{t_0, f_0}^l} = \partDiff{e}{x_{t_0}^l} \partDiff{x_{t_0}^l}{W_{t_0, f_0}^l}   $$
$$                              = \partDiff{e}{x_{t_0}^l} \partDiff{}{W_{t_0, f_0}^l} ( \sum_f W_{t_0, f}^l y_f^{l-1} ) $$
$$                              = \partDiff{e}{x_{t_0}^l} y_{f_0}^{l-1} $$

Or, in vector form: 

$$ \partDiff{e}{\mtrx{W}^l} = \partDiff{e}{\vec{x}^l} \outerp \vec{y}^{l-1} $$ 

So we should change the weigths by: 

$$ \Delta \mtrx{W}^l = - \alpha \partDiff{e}{\mtrx{W}^l} $$
$$ = \partDiff{e}{\vec{x}^l} \vec{y}^{l-1} $$


\paragraph{Gaussian processes}

The bayesian derivation of the backpropagation algorithm gives rise to the interesting idea that backprop-networks may be generalized into ...

\paragraph{Convolutional networks}

These are the networks most commonly found employed as a deep-learning architecture. Really, they are just a simplified version of our backpropagation-networks (they are even trained using an only slightly altered algorithm). Instead of connecting every node from layer $l$ to every node of layer $l+1$, they impose some restrictions on the connection-matrix:
\begin{itemize}
	\item Nodes are connected in a pyramid scheme. A node on layer $l+1$ is connected to 9 nodes directly beneath it. Instead of a $n_l \times n_{l+1}$ connection matrix, we thus have several $9 \times 1$ matrices.
	\item The connection-strengths of these  $9 \times 1$ matrices are all the same - so really there is only just one  $9 \times 1$ matrix. 
\end{itemize}
These restrictions are inspired by the physiology of the visual cortex. They have the nice effect that a network is trained much faster, since they massively reuce the ammount of weights that need to be learned. 

\subsection{Computer vision}

\paragraph{Feature detection and pose estimation}

Say you want to locate the position of the nose in a portrait. 
