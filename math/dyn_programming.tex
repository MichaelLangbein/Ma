\section{Dynamic Programming}

Dynamic programming tries to solve problems by expressing the problem recursively: The sollution for some parameter-set $sol(para)$ equals a function of sollutions of other parameter-sets, like for example: 
$$ sol(para) = sol(para') + x$$
or
$$ sol(para) = sol(para') + sol(para'')$$
or most generally:
$$ sol(para) = f(sol(para'), sol(para''), ...)$$

For performance, subsolutions are stored in a lookup-table. Without the lookup-table, dynamic programming would be called "divide and conquer".


Dynamic programming is a technique that solves some particular type of problem in polynomal time. Also, dp-sollutions can easily proven to be correct. But at first, we need to find out when dp-techniques apply. 

\subsection{Overlapping subproblems (always) and optimal substructure (most)}
\begin{itemize}
	\item Overlapping subproblems are those that make it worth to cache past results.
	\item Optimal substructure means this: if you have the sollution to all parts of the problem, you get the sollution to the whole problem. Consider a graph of nodes $a, b, c, d$. To get from $a$ to $d$, consider the subproblems of getting from $a$ to $b$ and from $b$ to $d$. The shortest path from $a$ to $d$ is the shortest path from $a$ to $b$ plus the shortest from $b$ to $d$. Note that the opposite problem does not have optimal substructure: the longest path from $a$ to $d$ is in fact shorter than the longest paths from $a$ to $b$ plus the longest  from $b$ to $d$.
\end{itemize}

\subsection{Worked example: possible ways to sum coins}
Imagine there was a currency with coins of value 1, 5 and 7. The task is to list all possible combinations of these basic coins to obtain a total value of $n$.


Here is an idea how this could be implemented: 
Lets call our function \inlinecode{combs(n)}.
Add 1 to each list in the results of \inlinecode{combs(n-1)},
add 5 to each list in the results of \inlinecode{combs(n-5)}, 
and add 7 to each list in the results of \inlinecode{combs(n-7)}.


\begin{lstlisting}[language=python]

def combs(n):
	if n < 1:
		return []
	if n == 1:
		return [[1]]
	if n == 5:
		return [[1,1,1,1,1],[5]]
	if n == 7:
		return [[1,1,1,1,1,1,1],[5,1,1],[1,5,1],[1,1,5],[7]]

	list1 = combs(n-1)
	list5 = combs(n-5)
	list7 = combs(n-7)

	fullList = []
	for c in list1:
		fullList.append(c + [1])
	for c in list5:
		fullList.append(c + [5])
	for c in list7: 
		fullList.append(c + [7])

	return fullList

\end{lstlisting}

Now just add some caching, and you have an efficient algorithm as well. 

Verify that this idea shows overlapping subproblems
and that it shows optimal substructure

Proof that algorithm delivers correct results


counterexample: minimal number of coins


\subsection{A strategy for finding the recursion}

Dynamic programming tries to solve problems by breaking them apart into subproblems. But how do we find those subproblems?

The sollution $S$ we are looking for should be obtained by some function $sol$.
$$ S = sol(para) $$

Try to find a way to split $S$ into a large part $S'$ and a small part $s$ \marginnote{In more advanced problems we might instead want to partition $S$ into several new sets $S'$ and $S''$}.
$$ S = S' + \{ s \} $$

The large part itself must be the result of $sol$ for some other set of parameters $para'$.
$$ S' = sol(para') $$


\subsection{Dynamic programming for hidden Markov-models (Viterbi algorithm)}
